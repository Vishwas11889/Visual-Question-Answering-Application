{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations of data and OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Please change the path of pdfs to be used for ingestion\n",
    "pdf_directory = r'C:\\Users\\User\\saf\\data'\n",
    "\n",
    "api_key = 'sk-ivoVnJsJad52iuO2E03VPTKaC9uJ3JZEKGySnkyVmtT3BlbkFJGeKJmBiELc-e0NAUFsGBnHb719qsxyalxEWj5-0F0A'\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting images to create summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting images to base64 encode\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Function to analyze image using OpenAI API\n",
    "def analyze_image(image_path):\n",
    "    base64_image = encode_image(image_path)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Analyze and summarize the image in detail\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1000\n",
    "    }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    data = response.json()\n",
    "    return data['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
    "all_summaries = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDF: C:\\Users\\User\\saf\\data\\Coding_challenge_source_1.pdf\n",
      "Analyzed Coding_challenge_source_1.pdf - page 1\n",
      "Processing PDF: C:\\Users\\User\\saf\\data\\Coding_challenge_source_2.pdf\n",
      "Analyzed Coding_challenge_source_2.pdf - page 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Processing PDF: {pdf_path}\")\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        pix = page.get_pixmap()\n",
    "        image_path = f'temp_page_{page_number + 1}.png'\n",
    "        pix.save(image_path)\n",
    "\n",
    "        content = analyze_image(image_path)\n",
    "        all_summaries += f\"Summary for {os.path.basename(pdf_path)} - page {page_number + 1}:\\n{content}\\n{'='*50}\\n\"\n",
    "        print(f\"Analyzed {os.path.basename(pdf_path)} - page {page_number + 1}\")\n",
    "        os.remove(image_path)\n",
    "\n",
    "    pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking and embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "final_documents = text_splitter.create_documents([all_summaries])\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectors = FAISS.from_documents(final_documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and execute the QA chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "qa_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate response based on the question.\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Questions: {input}\n",
    "\"\"\")\n",
    "document_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "retriever = vectors.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The instructions provided in the context are as follows:\n",
      "\n",
      "1. Complete the 3D model and drafting.\n",
      "2. Time: (to be filled in).\n",
      "3. Tool: CATIA V5 (any release).\n",
      "4. File name convention: Candidate name_ddmmyy.\n",
      "5. File path: Desktop with candidate folder name.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({'input': \"what are instructions\"})\n",
    "answer = response['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context is about detailed technical drawings of a mechanical component. These drawings include various views and dimensions necessary for manufacturing or modeling the part in 3D CAD software. The summaries highlight key aspects such as the dimensions of different segments, the presence of holes and notches, rounded corners, and specific details about different sections of the part. These technical drawings are essential for ensuring precision in mechanical design and serve as guides for creating accurate 3D models.\n"
     ]
    }
   ],
   "source": [
    "query = input('enter your query')\n",
    "response = retrieval_chain.invoke({'input': query})\n",
    "answer = response['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
